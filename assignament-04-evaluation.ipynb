{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f647c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "features = [     \n",
    "    'Make',\n",
    "    'Model',\n",
    "    'Year',\n",
    "    'Engine HP',\n",
    "    'Engine Cylinders',\n",
    "    'Transmission Type',\n",
    "    'Vehicle Style',\n",
    "    'highway MPG',\n",
    "    'city mpg',\n",
    "    'MSRP'\n",
    "]\n",
    "\n",
    "df = pd.read_csv('chapter04-eval-data.csv')\n",
    "df = df[ features ]\n",
    "df.columns = df.columns.str.replace(' ', '_').str.lower()\n",
    "df = df.fillna(0)\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e280d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priceMean =  40594.737032063116\n"
     ]
    }
   ],
   "source": [
    "priceMean = df['msrp'].mean()\n",
    "print( \"priceMean = \", priceMean)\n",
    "df[ 'above_average' ] = (df['msrp'] > priceMean ) + 0\n",
    "del df['msrp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7414c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=seed)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=seed)\n",
    "\n",
    "y_train = df_train[ 'above_average' ]\n",
    "y_val = df_val[ 'above_average' ]\n",
    "\n",
    "del df_train[ 'above_average' ]\n",
    "del df_val[ 'above_average' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59db4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and predict functions\n",
    "\n",
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categoricalFeatures + numericalFeatures].to_dict(orient='records')\n",
    "\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return dv, model\n",
    "\n",
    "def predict(df, dv, model):\n",
    "    dicts = df[categoricalFeatures + numericalFeatures].to_dict(orient='records')\n",
    "\n",
    "    X = dv.transform(dicts)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0a8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericalFeatures = [ 'year', 'engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg' ]\n",
    "categoricalFeatures = [ 'make', 'model', 'transmission_type', 'vehicle_style' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120c08f",
   "metadata": {},
   "source": [
    "Question 1: ROC AUC feature importance\n",
    "\n",
    "ROC AUC could also be used to evaluate feature importance of numerical variables.\n",
    "\n",
    "Let's do that\n",
    "\n",
    "    For each numerical variable, use it as score and compute AUC with the above_average variable\n",
    "    Use the training dataset for that\n",
    "\n",
    "If your AUC is < 0.5, invert this variable by putting \"-\" in front\n",
    "\n",
    "(e.g. -df_train['engine_hp'])\n",
    "\n",
    "AUC can go below 0.5 if the variable is negatively correlated with the target varialble. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive.\n",
    "\n",
    "Which numerical variable (among the following 4) has the highest AUC?\n",
    "\n",
    "    engine_hp\n",
    "    engine_cylinders\n",
    "    highway_mpg\n",
    "    city_mpg\n",
    "    \n",
    "ANSWER: engine_hp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd34ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature:  year  auc score:  0.5  auc:  0.5\n",
      "Feature:  engine_hp  auc score:  0.7944901528558269  auc:  0.7944901528558269\n",
      "Feature:  engine_cylinders  auc score:  0.6821317811693173  auc:  0.6821317811693173\n",
      "Feature:  highway_mpg  auc score:  0.5  auc:  0.5\n",
      "Feature:  city_mpg  auc score:  0.5012800819252432  auc:  0.5012800819252432\n"
     ]
    }
   ],
   "source": [
    "#dv = DictVectorizer(sparse=False)\n",
    "\n",
    "#train_dict = df_train[categoricalFeatures + numericalFeatures].to_dict(orient='records')\n",
    "#X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "for feature in numericalFeatures:\n",
    "    model = LogisticRegression()\n",
    "    model.fit(df_train[feature].to_frame(), y_train)\n",
    "    y_train_pred = model.predict(df_train[feature].to_frame())\n",
    "    #display( y_train )\n",
    "    #display(y_train_pred )\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, y_train_pred)\n",
    "    auc_score = roc_auc_score(y_train, y_train_pred)\n",
    "    auc_val = metrics.auc(fpr, tpr)\n",
    "    print( \"Feature: \", feature, \" auc score: \", auc_score, \" auc: \", auc_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cac7833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just an experiment\n"
     ]
    }
   ],
   "source": [
    "print( \"Just an experiment\")\n",
    "##for feature in numericalFeatures:\n",
    "    #y_train = df_train[ 'above_average' ]\n",
    "    #y_val = df_val[ 'above_average' ]\n",
    "\n",
    "    #del df_train[ 'above_average' ]\n",
    "    #del df_val[ 'above_average' ]\n",
    "    \n",
    "    ##dv, model = train(df_train[feature].to_frame(), y_train, C=1.0)\n",
    "    #y_train_pred = predict(df_train[feature].to_frame(), dv, model)\n",
    "    \n",
    "    #model = LogisticRegression()\n",
    "    #model.fit(df_train[feature].to_frame(), y_train)\n",
    "    #y_train_pred = model.predict(df_train[feature].to_frame())\n",
    "    #display( y_train )\n",
    "    #display(y_train_pred )\n",
    "    ##fpr, tpr, thresholds = roc_curve(y_train, y_train_pred)\n",
    "    ##auc_score = roc_auc_score(y_train, y_train_pred)\n",
    "    ##auc_val = metrics.auc(fpr, tpr)\n",
    "    ##print( \"Feature: \", feature, \" auc score: \", auc_score, \" auc: \", auc_val )\n",
    "    ##display( df_train[feature].to_frame() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558548d",
   "metadata": {},
   "source": [
    "Question 2: Training the model\n",
    "\n",
    "Apply one-hot-encoding using DictVectorizer and train the logistic regression with these parameters:\n",
    "\n",
    "LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "\n",
    "What's the AUC of this model on the validation dataset? (round to 3 digits)\n",
    "\n",
    "    0.678\n",
    "    0.779\n",
    "    0.878\n",
    "    0.979\n",
    "    \n",
    "ANSWER: 0.971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc040763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got X_train\n"
     ]
    }
   ],
   "source": [
    "train_dict = df_train[categoricalFeatures + numericalFeatures].to_dict(orient='records')\n",
    "\n",
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(train_dict)\n",
    "\n",
    "X_train = dv.transform(train_dict)\n",
    "print( \"Got X_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cba3c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got X_val\n"
     ]
    }
   ],
   "source": [
    "val_dict = df_val[categoricalFeatures + numericalFeatures].to_dict(orient='records')\n",
    "\n",
    "#dv = DictVectorizer(sparse=False)\n",
    "#dv.fit(val_dict)\n",
    "\n",
    "X_val = dv.transform(val_dict)\n",
    "print( \"Got X_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6872e7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_score:  0.915\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "auc_score = roc_auc_score(y_val, y_val_pred)\n",
    "print( \"auc_score: \", round( auc_score, 3 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a0af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_score:  0.971\n"
     ]
    }
   ],
   "source": [
    "# Variant using Alexey's functions\n",
    "dv, model = train(df_train, y_train, C=0.1)\n",
    "y_val_pred = predict(df_val, dv, model)\n",
    "\n",
    "auc_score = roc_auc_score(y_val, y_val_pred)\n",
    "print( \"auc_score: \", round( auc_score, 3 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2f71f",
   "metadata": {},
   "source": [
    "Question 3: Precision and Recall\n",
    "\n",
    "Now let's compute precision and recall for our model.\n",
    "\n",
    "    Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "    For each threshold, compute precision and recall\n",
    "    Plot them\n",
    "\n",
    "At which threshold precision and recall curves intersect?\n",
    "\n",
    "    0.28\n",
    "    0.48\n",
    "    0.68\n",
    "    0.88\n",
    " \n",
    "ANSWER:0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7720c509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND crt_c= 0.31  precision= 0.928  recall= 0.928\n",
      "FOUND crt_c= 0.33  precision= 0.929  recall= 0.929\n",
      "FOUND crt_c= 0.38  precision= 0.933  recall= 0.933\n",
      "FOUND crt_c= 0.4  precision= 0.934  recall= 0.934\n",
      "FOUND crt_c= 0.41  precision= 0.934  recall= 0.934\n",
      "FOUND crt_c= 0.42  precision= 0.935  recall= 0.935\n",
      "FOUND crt_c= 0.43  precision= 0.935  recall= 0.935\n",
      "FOUND crt_c= 0.44  precision= 0.934  recall= 0.934\n",
      "FOUND crt_c= 0.45  precision= 0.93  recall= 0.93\n",
      "FOUND crt_c= 0.49  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.5  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.54  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.56  precision= 0.933  recall= 0.933\n",
      "FOUND crt_c= 0.57  precision= 0.933  recall= 0.933\n",
      "FOUND crt_c= 0.58  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.61  precision= 0.938  recall= 0.938\n",
      "FOUND crt_c= 0.63  precision= 0.935  recall= 0.935\n",
      "FOUND crt_c= 0.64  precision= 0.935  recall= 0.935\n",
      "FOUND crt_c= 0.65  precision= 0.935  recall= 0.935\n",
      "FOUND crt_c= 0.66  precision= 0.938  recall= 0.938\n",
      "FOUND crt_c= 0.67  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.68  precision= 0.937  recall= 0.937\n",
      "FOUND IN LIST  0.68\n",
      "FOUND crt_c= 0.69  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.7  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.72  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.74  precision= 0.938  recall= 0.938\n",
      "FOUND crt_c= 0.75  precision= 0.932  recall= 0.932\n",
      "FOUND crt_c= 0.78  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.81  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.82  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.83  precision= 0.935  recall= 0.935\n",
      "FOUND crt_c= 0.87  precision= 0.935  recall= 0.935\n",
      "FOUND crt_c= 0.89  precision= 0.935  recall= 0.935\n",
      "FOUND crt_c= 0.9  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.91  precision= 0.932  recall= 0.932\n",
      "FOUND crt_c= 0.94  precision= 0.932  recall= 0.932\n",
      "FOUND crt_c= 0.95  precision= 0.937  recall= 0.937\n",
      "FOUND crt_c= 0.96  precision= 0.935  recall= 0.935\n",
      "FOUND crt_c= 0.98  precision= 0.936  recall= 0.936\n",
      "FOUND crt_c= 0.99  precision= 0.938  recall= 0.938\n"
     ]
    }
   ],
   "source": [
    "possible_answers = [0.28, 0.48, 0.68, 0.88]\n",
    "for crt_c in np.arange(0.01,1.0,0.01):\n",
    "    # avoid the rounding error\n",
    "    crt_c = round(crt_c,2)\n",
    "    model = LogisticRegression(solver='liblinear', C=crt_c, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    #fpr, tpr, thresholds = roc_curve(y_val, y_val_pred)\n",
    "    round_val = 3\n",
    "    precision = round( precision_score(y_val, y_val_pred, average='weighted'), round_val)\n",
    "    recall = round( recall_score(y_val, y_val_pred, average='weighted'), round_val)\n",
    "    if precision == recall:\n",
    "        print(\"FOUND crt_c=\", crt_c ,\" precision=\", precision, \" recall=\", recall )\n",
    "        if crt_c in possible_answers:\n",
    "            print(\"FOUND IN LIST \", crt_c) \n",
    "    #if precision > recall:\n",
    "    #    print(\" P>R crt_c=\", crt_c ,\" precision=\", precision, \" recall=\", recall )\n",
    "    #else:\n",
    "    #    print(\"P<R crt_c=\", crt_c,\" precision=\", precision, \" recall=\", recall )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83caf27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant using Alexey's functions\n",
    "\n",
    "possible_answers = [0.28, 0.48, 0.68, 0.88]\n",
    "for crt_c in np.arange(0.01,1.0,0.01):\n",
    "    # avoid the rounding error\n",
    "    crt_c = round(crt_c,2)\n",
    "    \n",
    "    print(crt_c\n",
    "    dv, model = train(df_train, y_train, C=crt_c)\n",
    "    y_val_pred = predict(df_val, dv, model)\n",
    "    \n",
    "    #print( y_val_pred)\n",
    "    ###model = LogisticRegression(solver='liblinear', C=crt_c, max_iter=1000)\n",
    "    ###model.fit(X_train, y_train)\n",
    "    ###y_val_pred = model.predict(X_val)\n",
    "    #fpr, tpr, thresholds = roc_curve(y_val, y_val_pred)\n",
    "    \n",
    "    precision = precision_score(y_val, y_val_pred >= 0.5, average='weighted')\n",
    "    recall = recall_score(y_val, y_val_pred >= 0.5, average='weighted')\n",
    "    \n",
    "    #round_val = 16\n",
    "    #precision = round( precision, round_val )\n",
    "    #recall = round( precision, round_val )\n",
    "    if precision == recall:\n",
    "        print(\"FOUND crt_c=\", crt_c ,\" precision=\", precision, \" recall=\", recall )\n",
    "        if crt_c in possible_answers:\n",
    "            print(\"FOUND IN LIST \", crt_c) \n",
    "    #print( accuracy_score(y_val, y_val_pred >= 0.5) )\n",
    "#print(\"Done this step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a0154",
   "metadata": {},
   "source": [
    "Question 4: F1 score\n",
    "\n",
    "Precision and recall are conflicting - when one grows, the other goes down. That's why they are often combined into the F1 score - a metrics that takes into account both\n",
    "\n",
    "This is the formula for computing F1:\n",
    "        \n",
    "        F1 = 2 * P * R/ (P + R)\n",
    "\n",
    "Where P is precision and R is recall.\n",
    "\n",
    "Let's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01\n",
    "\n",
    "At which threshold F1 is maximal?\n",
    "\n",
    "    0.12\n",
    "    0.32\n",
    "    0.52\n",
    "    0.72\n",
    "\n",
    "ANSWER: 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb59927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crt_c= 0.12  F1= 0.9185335193169534\n",
      "crt_c= 0.32  F1= 0.9283927894250757\n",
      "crt_c= 0.52  F1= 0.9305310271935054\n",
      "crt_c= 0.72  F1= 0.9372737196769443\n",
      "maxF1= 0.9385464589421831  cForMaxF= 0.6\n"
     ]
    }
   ],
   "source": [
    "possible_answers = [0.12, 0.32, 0.52, 0.72]\n",
    "maxF1 = 0\n",
    "cForMaxF = 0\n",
    "for crt_c in np.arange(0.01,1.0,0.01):\n",
    "    # avoid the rounding error\n",
    "    crt_c = round(crt_c,2) + 0.0\n",
    "    model = LogisticRegression(solver='liblinear', C=crt_c, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    #fpr, tpr, thresholds = roc_curve(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred, average='weighted')\n",
    "    recall = recall_score(y_val, y_val_pred, average='weighted')\n",
    "    round_val = 6\n",
    "    #precision = round( precision, round_val)\n",
    "    #recall = round( recall, round_val)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    if F1 >= maxF1:\n",
    "        #print( \"Found new max F1; maxF1=\", F1, \" cForMaxF=\",crt_c )\n",
    "        #if crt_c in possible_answers:\n",
    "        #    print(\"FOUND IN LIST \", crt_c)\n",
    "        if F1 > maxF1:\n",
    "            maxF1 = F1\n",
    "            cForMaxF = crt_c\n",
    "    if crt_c in possible_answers:\n",
    "        print( \"crt_c=\", crt_c, \" F1=\", F1 )\n",
    "print( \"maxF1=\", maxF1, \" cForMaxF=\",cForMaxF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56a12d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crt_c= 0.12  F1= 0.9185335193169534\n",
      "crt_c= 0.32  F1= 0.9283927894250757\n",
      "crt_c= 0.52  F1= 0.9305310271935054\n",
      "crt_c= 0.72  F1= 0.9372737196769443\n",
      "maxF1= 0.9385464589421831  cForMaxF= 0.6\n"
     ]
    }
   ],
   "source": [
    "# Variant using Alexey's functions\n",
    "\n",
    "possible_answers = [0.12, 0.32, 0.52, 0.72]\n",
    "maxF1 = 0\n",
    "cForMaxF = 0\n",
    "for crt_c in np.arange(0.01,1.0,0.01):\n",
    "    # avoid the rounding error\n",
    "    crt_c = round(crt_c,2) + 0.0\n",
    "    \n",
    "    dv, model = train(df_train, y_train, C=crt_c)\n",
    "    y_val_pred = predict(df_val, dv, model)\n",
    "    \n",
    "    precision = precision_score(y_val, y_val_pred >= 0.5 , average='weighted')\n",
    "    recall = recall_score(y_val, y_val_pred >= 0.5, average='weighted')\n",
    "    round_val = 6\n",
    "    #precision = round( precision, round_val)\n",
    "    #recall = round( recall, round_val)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    if F1 >= maxF1:\n",
    "        #print( \"Found new max F1; maxF1=\", F1, \" cForMaxF=\",crt_c )\n",
    "        #if crt_c in possible_answers:\n",
    "        #    print(\"FOUND IN LIST \", crt_c)\n",
    "        if F1 > maxF1:\n",
    "            maxF1 = F1\n",
    "            cForMaxF = crt_c\n",
    "    if crt_c in possible_answers:\n",
    "        print( \"crt_c=\", crt_c, \" F1=\", F1 )\n",
    "print( \"maxF1=\", maxF1, \" cForMaxF=\",cForMaxF )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a34bf",
   "metadata": {},
   "source": [
    "Question 5: 5-Fold CV\n",
    "\n",
    "Use the KFold class from Scikit-Learn to evaluate our model on 5 different folds:\n",
    "\n",
    "KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    Iterate over different folds of df_full_train\n",
    "    Split the data into train and validation\n",
    "    Train the model on train with these parameters: LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "    Use AUC to evaluate the model on validation\n",
    "\n",
    "How large is standard devidation of the scores across different folds?\n",
    "\n",
    "    0.003\n",
    "    0.030\n",
    "    0.090\n",
    "    0.140\n",
    "    \n",
    "ANSWER: 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84ee58f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979 +- 0.002\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "scores = []\n",
    "C=1.0\n",
    "\n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "    y_train = df_train[ 'above_average' ]\n",
    "    y_val = df_val[ 'above_average' ]\n",
    "\n",
    "    del df_train[ 'above_average' ]\n",
    "    del df_val[ 'above_average' ]\n",
    "\n",
    "    \n",
    "    dv, model = train(df_train, y_train, C=C)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "print('%.3f +- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36bee64",
   "metadata": {},
   "source": [
    "Question 6: Hyperparemeter Tuning\n",
    "\n",
    "Now let's use 5-Fold cross-validation to find the best parameter C\n",
    "\n",
    "    Iterate over the following C values: [0.01, 0.1, 0.5, 10]\n",
    "    Initialize KFold with the same parameters as previously\n",
    "    Use these parametes for the model: LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "    Compute the mean score as well as the std (round the mean and std to 3 decimal digits)\n",
    "\n",
    "Which C leads to the best mean score?\n",
    "\n",
    "    0.01\n",
    "    0.1\n",
    "    0.5\n",
    "    10\n",
    "\n",
    "If you have ties, select the score with the lowest std. If you still have ties, select the smallest C.\n",
    "\n",
    "ANSWER: C = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "867faf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01 0.952 +- 0.003\n",
      "C=0.1 0.972 +- 0.002\n",
      "C=0.5 0.977 +- 0.002\n",
      "C=10 0.981 +- 0.003\n"
     ]
    }
   ],
   "source": [
    "cList = [0.01, 0.1, 0.5, 10]\n",
    "\n",
    "for C in cList:\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(df_full_train):\n",
    "        df_train = df_full_train.iloc[train_idx]\n",
    "        df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "        y_train = df_train[ 'above_average' ]\n",
    "        y_val = df_val[ 'above_average' ]\n",
    "\n",
    "        del df_train[ 'above_average' ]\n",
    "        del df_val[ 'above_average' ]\n",
    "\n",
    "    \n",
    "        dv, model = train(df_train, y_train, C=C)\n",
    "        y_pred = predict(df_val, dv, model)\n",
    "\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(auc)\n",
    "\n",
    "    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24c94b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
